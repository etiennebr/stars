# Analyzing Spatiotemporal Tidy Arrays with R

[Edzer Pebesma](https://github.com/edzer/), [Michael
Sumner](https://github.com/mdsumer/), [Etienne
Racine](https://github.com/etiennebr)

## Summary

A lot of spatiotemporal data takes the form of dense,
multidimensional arrays:

* population counts by region, year and age group 
* meteorological data by variable, time step and sensor location 
* satellite imagery, e.g. energy by color, location and time step
* or climate data (e.g. surface temperature by location, time and climate scenario). 

Although such data _can_ be represented in long tables, it is
rarely useful for larger datasets because it requires to replicate
dimension indexes, and because the array form provides faster
access. R's native arrays can't handle data records of mixed
type like we typically find in data.frame's, can only deal
with in-memory data, and does not handle spatial or temporal
array dimensions. This project will (i) implement a flexible
and generic array model for heterogeneous records that (ii) can
handle strong spatial and temporal referencing of array indexes,
and (iii) can scale from in memory,
to on disk, and to s remote server while
using a unified user interface that follows the [tidy tools
manifesto](https://cran.r-project.org/web/packages/tidyverse/vignettes/manifesto.html).

Now that [simple features for R](https://github.com/edzer/sfr) has provided a large modernizing of the handling and analysis vector data (points, lines, polygons) in R, it is time for raster data to catch up. This proposal aims at 2D/3D temporal rasters, as well as time series of feature data.

_Italics: the [R Consortium call](https://www.r-consortium.org/projects/call-for-proposals); deadline [Feb 10, 2017](https://www.r-consortium.org/blog/2016/12/06/call-for-proposals)_

## The Problem

_What problem do you want to solve? Why is it a problem? Who does it affect? What will solving the problem enable? This section should include a brief summary of existing work, such as R packages that may be relevant._

How do we handle and analyze large amounts of spatially referenced
time series data in R? How do we handle satellite imagery that
don't fit on a local disc, with R, or for which we need a cluster
to finish computation in an acceptable time? How can we quickly
and easily develop an analysis by testing it on a small portion of
the spatiotemporal datasets, before deploying it on the massive
data set?  How can we use pipe-based workflows or dplyr-verbs on
such data sets? Many users use R for large spatiotemporal data,
but hit limits related to usability, user interface, and scalability.

### Existing work

Base R supports n-dimensional homogeneous arrays of basic
types (double, integer, logical, logical), in-memory. Package
[ff](https://CRAN.R-project.org/package=ff) supports out-of-memory
datastructures, held on local disc, but no spatial or temporal
references on dimensions.

Spatial packages include
[rgdal](https://CRAN.R-project.org/package=ff),
which lets you read and write raster data, possibly
piece-by-piece, in one of 142 different file formats. Package
[raster](https://CRAN.R-project.org/package=raster) allows users
to work with raster maps or stacks of them, where a stack can
either refer to different bands (color) or different time staps,
but not both. Package raster can iterate functions over large
files on disc, and takes care of the caching, using either rgdal or
[ncdf4](https://CRAN.R-project.org/package=ncdf4).  Packages that
are more dedicated to a single data source or type include include
[RStoolbox](https://CRAN.R-project.org/package=RStoolbox),
[MODIS](https://CRAN.R-project.org/package=MODIS),
[landsat](https://CRAN.R-project.org/package=landsat), and
[hsdar](https://CRAN.R-project.org/package=hsdar); each of these
relies on raster or rgdal for file-based I/O.

CRAN package
[spacetime](https://CRAN.R-project.org/package=spacetime) provides
heterogeneous records (data.frame), and keeps indexes for each
record to a spatial geometry (grid cell/point/polygon) and time
instance or period; it keeps all data in memory and builds on
[xts](https://CRAN.R-project.org/package=xts) for time and
[sp](https://CRAN.R-project.org/package=sp) for space.


Relevant work outside R includes
* [GDAL](http://www.gdal.org/), in particular gdal [virtual tiles](http://www.gdal.org/gdalbuildvrt.html)
* [SciDB](http://www.paradigm4.com/), an open source array database which has no spatial or temporal capabilities
* [SciDB4geo](https://github.com/appelmar/scidb4geo), a SciDB Plugin
for Managing Spatial and Temporal Reference Information of Arrays, and
[SciDB4gdal](https://github.com/appelmar/scidb4geo), a GDAL driver for SciDB arrays, two activities to make SciDB databases aware of space and time
* [PostGIS Raster](http://postgis.net/docs/RT_reference.html), a raster data extension of [PostGIS](http://www.postgis.net/)
* [Rasdaman](http://www.rasdaman.com/), an array database dedicate to images, only partially open source.

Since there is a definite trend that
[downloading Earth observation data is no longer
feasible](http://r-spatial.org/2016/11/29/openeo.html),
we will have to work towards a solution where the data are
accessed over a web service interface, for instance using
[opencpu](http://www.opencpu.org/). The current mainstream packages
rgdal, raster, and ncdf4 need a successor.

## The Plan: 

_How are you going to solve the problem? Include the concrete actions you will take and an estimated timeline. What are likely failure modes and how will you recover from them?_

We will develop an R package and container infrastructure that 
* supports dense arrays with heterogeneous records, 
* supports flexible reference from array dimensions to space, where space can be gridded (2D/3D raster) or a set of simple features (irregular)
* supports flexible reference from array dimensions time (POSIXct, Date)
* allow for regular arrays (fixed cell size / time step) and irregular arrays
* allows working in memory, on local disc, and on a remote computer (web service),
* allows R functions to be passed on to the web service back-end, and executed there (potentially in parallel),
* uses in-memory proxies to large arrays, allowing to work with the first _n_ records before computations are carried out on the full arrays (similar to how dplyr does this)
* allows pipe-based workflows, and uses data.frame's and dplyr-style verbs.

We will document the software and provide tutorials and reproducible
data analysis examples using locally downloaded imagery, as well as
scalable examples accessing larger (> 1 Tb) datasets using docker
container images.

We will document the RESTful API that will connect the R client with
the web service holding (and processing) the big Earth observation
data.

We will also develop a migration path for the raster package (over
43K lines of R, C and C++ code), and its functionality, into the
new infrastructure.

We will publish the resulting products in an open access form,
in the R journal, but also in a journal (or on a conference) more
directed to the Earth observation community.

Timeline:
* Month 1-2: work out design, decide web service technology, basic web service API design
* Month 3-6: programming the R package, testing with smaller data sets
* Month 7-8: testing on larger datasets, develop test cases, deploy on docker containers
* Month 9-12: write tutorials, develop teaching material and reproducible examples
* Month 9-12: experiment with different back-ends: file-based, or database such as SciDB

Failure modes:
* we can't get the RESTful API to work properly; solution path: ask the rOpenSci people for help (Scott Chamberlain,
Jeroen Ooms)
* downloading large image sets is too cumbersome (slow) for the larger tutorial examples; solution path: deploy a test server for teaching/experimenting purposes in the Amazon cloud, where Landsat and Sentinel imagery is readily available

## How Can The ISC Help: 

_Please describe how you think the ISC can help. If you are looking for a cash grant include a detailed itemised budget and spending plan. We expect that most of the budget will be allocated for people, but we will consider funding travel, equipment and services, such as cloud computing resources with good justification. If you are seeking to start an ISC working group, then please describe the goals of the group and provide the name of the individual who will be committed to leading and managing the groupâ€™s activities. Also describe how you think the ISC can we help promote your project._

We will use most funding to actually develop the R package and web service API. Total costs are estimated at 10,000 USD, and break down in:
* workshop: travel costs for Etienne Racine and Michael Sumner to visit Muenster, or another venue (USD 2500).
* Programming, project communication: (USD 7000).
* Cloud deployment in the Amazon cloud (USD 500).

## Dissemination: 

_How will you ensure that your work is available to the widest number of people? Please specify the open source license will you use, how you will host your code so that others can contribute, and how you will publicise your work. We encourage you to plan at least two blog posts to the R consortium blog: one to announce the project, and one to write up what you achieved._

We will blog post about the project on [r-spatial.org](http://r-spatial.org/), use twitter, post to [r-sig-geo](https://stat.ethz.ch/mailman/listinfo/r-sig-geo), stackoverflow, and communicate through github issues. The project will involve on github, most likely in the [r-spatial](https://github.com/r-spatial/) organisation.We will work under a permissive open source license, probably LGPL-2.1.  Pull requests will be encouraged. R consortium blogs will be provided at start and end. Publications in _the R Journal_ and scientific outlets are foreseen.

